{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZdKI6NA_9KVH",
        "outputId": "1b3d7b94-ed2a-405c-de41-ee6901655ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.47.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=94670e28e0321d88c9850578a63152113783508fbfd64ea10cf36d1e38620a11\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting sinling\n",
            "  Downloading sinling-0.3.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting emoji (from sinling)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sinling) (1.5.1)\n",
            "Collecting pygtrie (from sinling)\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting sklearn-crfsuite (from sinling)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from sinling) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->sinling) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->sinling) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->sinling) (4.67.1)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->sinling)\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite->sinling) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite->sinling) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (3.6.0)\n",
            "Downloading sinling-0.3.6-py3-none-any.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygtrie, python-crfsuite, emoji, sklearn-crfsuite, sinling\n",
            "Successfully installed emoji-2.14.1 pygtrie-2.5.0 python-crfsuite-0.9.11 sinling-0.3.6 sklearn-crfsuite-0.5.0\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.12/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.4)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (25.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.8.3)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              },
              "id": "9435a81656e84d4393e0aa8ae5df0506"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn nltk\n",
        "!pip install transformers torch\n",
        "!pip install fastapi uvicorn\n",
        "!pip install langdetect\n",
        "!pip install sinling  # For Sinhala text processing\n",
        "!pip install indic-nlp-library  # For Tamil text processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Language detection\n",
        "from langdetect import detect\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW1jpqt_9NPG",
        "outputId": "20e8e6d6-37b4-4b7e-c86b-ddd726221634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df = pd.read_excel('/mnt/data/cleaned_phishing_data.xlsx')\n",
        "    print(\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading dataset: {e}\")\n",
        "    print(\"Please upload your Excel file to Colab first\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yETgtAPB9vWV",
        "outputId": "431b3ef2-b642-49a2-dfef-a20887a45997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded successfully!\n",
            "Dataset shape: (300, 2)\n",
            "Columns: ['Email Text', 'Email Type']\n",
            "\n",
            "First few rows:\n",
            "                                          Email Text Email Type\n",
            "0  Dear Jordan, your subscription has been succes...       safe\n",
            "1  ‡∂Ü‡∂Ø‡∂ª‡∂´‡∑ì‡∂∫ Jordan, ‡∂î‡∂∂‡∂ú‡∑ö ‡∂Ø‡∑è‡∂∫‡∂ö‡∂≠‡∑ä‡∑Ä‡∂∫ ‡∑É‡∑è‡∂ª‡∑ä‡∂Æ‡∂ö‡∑Ä ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂∫‡∑è‡∑Ä‡∂≠...       safe\n",
            "2  ‡ÆÖ‡Æ©‡Øç‡Æ™‡ØÅ‡Æ≥‡Øç‡Æ≥ Jordan, ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æö‡Æ®‡Øç‡Æ§‡Ææ ‡Æµ‡ØÜ‡Æ±‡Øç‡Æ±‡Æø‡Æï‡Æ∞‡ÆÆ‡Ææ‡Æï ‡Æ™‡ØÅ‡Æ§‡ØÅ...       safe\n",
            "3  Dear Casey, thank you for your purchase. Your ...       safe\n",
            "4  ‡∂Ü‡∂Ø‡∂ª‡∂´‡∑ì‡∂∫ Casey, ‡∂î‡∂∂‡∂ú‡∑ö ‡∂∏‡∑í‡∂Ω‡∂Ø‡∑ì ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏ ‡∑É‡∂≥‡∑Ñ‡∑è ‡∑É‡∑ä‡∂≠‡∑î‡∂≠‡∑í‡∂∫‡∑í. ...       safe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilingualTextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.english_stopwords = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',\n",
        "                                     'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'])\n",
        "\n",
        "        # Sinhala stopwords (common words to remove)\n",
        "        self.sinhala_stopwords = set(['‡∂∏‡∂∏', '‡∂î‡∂∂', '‡∂î‡∑Ñ‡∑î', '‡∂á‡∂∫', '‡∂Ö‡∂¥‡∑í', '‡∂î‡∂∂‡∂ß', '‡∂î‡∂∂‡∂ú‡∑ö', '‡∂∏‡∂ú‡∑ö',\n",
        "                                     '‡∂ë‡∂∏', '‡∂∏‡∑ô‡∂∏', '‡∑É‡∑Ñ', '‡∂±‡∂∏‡∑î‡∂≠‡∑ä', '‡∑Ñ‡∑ù', '‡∂Ø', '‡∂ß', '‡∂ú‡∑ö', '‡∂∫‡∂±'])\n",
        "\n",
        "        # Tamil stopwords (common words to remove)\n",
        "        self.tamil_stopwords = set(['‡Æ®‡Ææ‡Æ©‡Øç', '‡Æ®‡ØÄ', '‡ÆÖ‡Æµ‡Æ©‡Øç', '‡ÆÖ‡Æµ‡Æ≥‡Øç', '‡Æ®‡Ææ‡ÆÆ‡Øç', '‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç', '‡Æé‡Æ©‡Øç',\n",
        "                                   '‡ÆÖ‡Æ®‡Øç‡Æ§', '‡Æá‡Æ®‡Øç‡Æ§', '‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç', '‡ÆÜ‡Æ©‡Ææ‡Æ≤‡Øç', '‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ', '‡Æâ‡Æ≥‡Øç‡Æ≥', '‡Æï‡Øç‡Æï‡ØÅ'])\n",
        "\n",
        "    def detect_language(self, text):\n",
        "        \"\"\"Detect language of text\"\"\"\n",
        "        try:\n",
        "            # Check for Sinhala characters\n",
        "            if re.search(r'[\\u0D80-\\u0DFF]', text):\n",
        "                return 'sinhala'\n",
        "            # Check for Tamil characters\n",
        "            elif re.search(r'[\\u0B80-\\u0BFF]', text):\n",
        "                return 'tamil'\n",
        "            # Default to English\n",
        "            else:\n",
        "                return 'english'\n",
        "        except:\n",
        "            return 'english'\n",
        "\n",
        "    def clean_text(self, text, language):\n",
        "        \"\"\"Clean text based on language\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text).strip()\n",
        "\n",
        "        if language == 'english':\n",
        "            # English preprocessing\n",
        "            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' URL ', text)\n",
        "            text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' URL ', text)\n",
        "            text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', ' EMAIL ', text)\n",
        "            text = re.sub(r'\\b\\d+\\b', ' NUMBER ', text)\n",
        "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "            text = re.sub(r'\\s+', ' ', text)\n",
        "            text = text.lower()\n",
        "\n",
        "        elif language == 'sinhala':\n",
        "            # Sinhala preprocessing\n",
        "            text = re.sub(r'http[s]?://\\S+', ' URL ', text)\n",
        "            text = re.sub(r'www\\.\\S+', ' URL ', text)\n",
        "            text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', ' EMAIL ', text)\n",
        "            text = re.sub(r'[0-9]+', ' NUMBER ', text)\n",
        "            # Keep Sinhala characters, spaces, and basic punctuation\n",
        "            text = re.sub(r'[^\\u0D80-\\u0DFF\\s]', ' ', text)\n",
        "            text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        elif language == 'tamil':\n",
        "            # Tamil preprocessing\n",
        "            text = re.sub(r'http[s]?://\\S+', ' URL ', text)\n",
        "            text = re.sub(r'www\\.\\S+', ' URL ', text)\n",
        "            text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', ' EMAIL ', text)\n",
        "            text = re.sub(r'[0-9]+', ' NUMBER ', text)\n",
        "            # Keep Tamil characters, spaces, and basic punctuation\n",
        "            text = re.sub(r'[^\\u0B80-\\u0BFF\\s]', ' ', text)\n",
        "            text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def remove_stopwords(self, text, language):\n",
        "        \"\"\"Remove stopwords based on language\"\"\"\n",
        "        words = text.split()\n",
        "\n",
        "        if language == 'english':\n",
        "            words = [word for word in words if word.lower() not in self.english_stopwords]\n",
        "        elif language == 'sinhala':\n",
        "            words = [word for word in words if word not in self.sinhala_stopwords]\n",
        "        elif language == 'tamil':\n",
        "            words = [word for word in words if word not in self.tamil_stopwords]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        if pd.isna(text) or text == \"\":\n",
        "            return \"\", \"unknown\"\n",
        "\n",
        "        # Detect language\n",
        "        language = self.detect_language(text)\n",
        "\n",
        "        # Clean text\n",
        "        cleaned_text = self.clean_text(text, language)\n",
        "\n",
        "        # Remove stopwords\n",
        "        final_text = self.remove_stopwords(cleaned_text, language)\n",
        "\n",
        "        return final_text, language\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = MultilingualTextPreprocessor()"
      ],
      "metadata": {
        "id": "tboQWF0a907D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(df):\n",
        "    \"\"\"Prepare the dataset for training\"\"\"\n",
        "\n",
        "    # Print original columns to debug\n",
        "    print(f\"Original columns: {list(df.columns)}\")\n",
        "    print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "    # Rename columns for consistency (adjust based on your dataset)\n",
        "    if 'Email Text' in df.columns:\n",
        "        df = df.rename(columns={'Email Text': 'text'})\n",
        "    if 'Email Type' in df.columns:\n",
        "        df = df.rename(columns={'Email Type': 'label'})\n",
        "\n",
        "    # Print unique labels to see what we have\n",
        "    print(f\"Unique labels before processing: {df['label'].unique()}\")\n",
        "\n",
        "    # Clean the dataset\n",
        "    df = df.dropna(subset=['text', 'label'])\n",
        "\n",
        "    # Standardize labels - handle both 'safe/unsafe' and 'legitimate/scam'\n",
        "    df['label'] = df['label'].astype(str).str.lower().str.strip()\n",
        "\n",
        "    # Map different label formats to standard format\n",
        "    label_mapping = {\n",
        "        'safe': 'legitimate',\n",
        "        'unsafe': 'scam',\n",
        "        'legitimate': 'legitimate',\n",
        "        'scam': 'scam',\n",
        "        '0': 'legitimate',  # In case labels are numeric\n",
        "        '1': 'scam',\n",
        "        0: 'legitimate',\n",
        "        1: 'scam'\n",
        "    }\n",
        "\n",
        "    df['label'] = df['label'].map(label_mapping)\n",
        "\n",
        "    # Keep only valid labels\n",
        "    df = df[df['label'].isin(['legitimate', 'scam'])]\n",
        "\n",
        "    print(f\"Labels after mapping: {df['label'].value_counts()}\")\n",
        "\n",
        "    # Preprocess text and detect language\n",
        "    processed_data = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        text = row['text']\n",
        "        label = row['label']\n",
        "\n",
        "        if pd.isna(text) or str(text).strip() == '':\n",
        "            continue\n",
        "\n",
        "        # Preprocess text\n",
        "        processed_text, detected_language = preprocessor.preprocess(str(text))\n",
        "\n",
        "        if processed_text.strip():  # Only include non-empty texts\n",
        "            processed_data.append({\n",
        "                'original_text': text,\n",
        "                'processed_text': processed_text,\n",
        "                'language': detected_language,\n",
        "                'label': label\n",
        "            })\n",
        "\n",
        "    processed_df = pd.DataFrame(processed_data)\n",
        "\n",
        "    print(f\"‚úÖ Dataset processed successfully!\")\n",
        "    print(f\"Total samples: {len(processed_df)}\")\n",
        "    print(f\"Language distribution:\")\n",
        "    print(processed_df['language'].value_counts())\n",
        "    print(f\"Label distribution:\")\n",
        "    print(processed_df['label'].value_counts())\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "# Process your dataset\n",
        "processed_df = prepare_dataset(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUyUttrZ98cj",
        "outputId": "80fcbaa0-9345-46a3-bfdb-87be3fa868c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original columns: ['Email Text', 'Email Type']\n",
            "Original shape: (300, 2)\n",
            "Unique labels before processing: ['safe' 'unsafe']\n",
            "Labels after mapping: label\n",
            "legitimate    231\n",
            "scam           69\n",
            "Name: count, dtype: int64\n",
            "‚úÖ Dataset processed successfully!\n",
            "Total samples: 300\n",
            "Language distribution:\n",
            "language\n",
            "english    100\n",
            "sinhala    100\n",
            "tamil      100\n",
            "Name: count, dtype: int64\n",
            "Label distribution:\n",
            "label\n",
            "legitimate    231\n",
            "scam           69\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilingualScamDetector:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.vectorizers = {}\n",
        "        self.preprocessor = MultilingualTextPreprocessor()\n",
        "\n",
        "    def train_language_model(self, texts, labels, language):\n",
        "        \"\"\"Train a model for specific language\"\"\"\n",
        "        print(f\"Training model for {language}...\")\n",
        "\n",
        "        # Create TF-IDF vectorizer with language-specific parameters\n",
        "        if language == 'english':\n",
        "            vectorizer = TfidfVectorizer(\n",
        "                max_features=5000,\n",
        "                ngram_range=(1, 2),\n",
        "                min_df=2,\n",
        "                max_df=0.8\n",
        "            )\n",
        "        else:  # Sinhala and Tamil\n",
        "            vectorizer = TfidfVectorizer(\n",
        "                max_features=3000,\n",
        "                ngram_range=(1, 3),\n",
        "                min_df=1,\n",
        "                max_df=0.9,\n",
        "                analyzer='char'  # Character-level analysis\n",
        "            )\n",
        "\n",
        "        # Vectorize texts\n",
        "        X = vectorizer.fit_transform(texts)\n",
        "\n",
        "        # Train model\n",
        "        model = LogisticRegression(\n",
        "            random_state=42,\n",
        "            max_iter=1000,\n",
        "            class_weight='balanced'  # Handle imbalanced data\n",
        "        )\n",
        "        model.fit(X, labels)\n",
        "\n",
        "        # Store model and vectorizer\n",
        "        self.models[language] = model\n",
        "        self.vectorizers[language] = vectorizer\n",
        "\n",
        "        # Get feature names for keyword extraction\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        # Print training results\n",
        "        y_pred = model.predict(X)\n",
        "        accuracy = accuracy_score(labels, y_pred)\n",
        "        print(f\"‚úÖ {language.title()} model trained - Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def train_all_models(self, processed_df):\n",
        "        \"\"\"Train models for all languages\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for language in processed_df['language'].unique():\n",
        "            if language == 'unknown':\n",
        "                continue\n",
        "\n",
        "            # Filter data for this language\n",
        "            lang_data = processed_df[processed_df['language'] == language]\n",
        "\n",
        "            if len(lang_data) < 10:  # Skip if too few samples\n",
        "                print(f\"‚ö†Ô∏è Skipping {language} - insufficient data ({len(lang_data)} samples)\")\n",
        "                continue\n",
        "\n",
        "            # Get texts and labels\n",
        "            texts = lang_data['processed_text'].tolist()\n",
        "            labels = lang_data['label'].tolist()\n",
        "\n",
        "            # Train model\n",
        "            accuracy = self.train_language_model(texts, labels, language)\n",
        "            results[language] = accuracy\n",
        "\n",
        "        print(f\"\\nüéâ Training completed for all languages!\")\n",
        "        return results\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"Predict if text is scam or legitimate\"\"\"\n",
        "        # Preprocess text\n",
        "        processed_text, language = self.preprocessor.preprocess(text)\n",
        "\n",
        "        if not processed_text.strip():\n",
        "            return {\n",
        "                'text': text,\n",
        "                'language': 'unknown',\n",
        "                'classification': 'unknown',\n",
        "                'risk_score': 0,\n",
        "                'suspicious_terms': [],\n",
        "                'explanation': 'Unable to process text'\n",
        "            }\n",
        "\n",
        "        # Check if we have a model for this language\n",
        "        if language not in self.models:\n",
        "            # Try English model as fallback\n",
        "            if 'english' in self.models:\n",
        "                language = 'english'\n",
        "                processed_text, _ = self.preprocessor.preprocess(text)\n",
        "            else:\n",
        "                return {\n",
        "                    'text': text,\n",
        "                    'language': language,\n",
        "                    'classification': 'unknown',\n",
        "                    'risk_score': 0,\n",
        "                    'suspicious_terms': [],\n",
        "                    'explanation': f'No model available for {language}'\n",
        "                }\n",
        "\n",
        "        # Get model and vectorizer\n",
        "        model = self.models[language]\n",
        "        vectorizer = self.vectorizers[language]\n",
        "\n",
        "        # Vectorize text\n",
        "        X = vectorizer.transform([processed_text])\n",
        "\n",
        "        # Predict\n",
        "        prediction = model.predict(X)[0]\n",
        "        probability = model.predict_proba(X)[0]\n",
        "\n",
        "        # Get risk score (probability of being scam)\n",
        "        scam_prob = probability[1] if model.classes_[1] == 'scam' else probability[0]\n",
        "        risk_score = int(scam_prob * 100)\n",
        "\n",
        "        # Get suspicious terms (top features with highest coefficients)\n",
        "        suspicious_terms = self.get_suspicious_terms(vectorizer, model, processed_text)\n",
        "\n",
        "        # Generate explanation\n",
        "        explanation = self.generate_explanation(prediction, risk_score, suspicious_terms, language)\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'language': language,\n",
        "            'classification': prediction,\n",
        "            'risk_score': risk_score,\n",
        "            'suspicious_terms': suspicious_terms,\n",
        "            'explanation': explanation\n",
        "        }\n",
        "\n",
        "    def get_suspicious_terms(self, vectorizer, model, text, top_n=3):\n",
        "        \"\"\"Extract suspicious terms from text\"\"\"\n",
        "        try:\n",
        "            # Get feature names\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "            # Get model coefficients\n",
        "            if hasattr(model, 'coef_'):\n",
        "                coefficients = model.coef_[0]\n",
        "            else:\n",
        "                return []\n",
        "\n",
        "            # Vectorize the text\n",
        "            X = vectorizer.transform([text])\n",
        "\n",
        "            # Get non-zero features (words present in text)\n",
        "            feature_indices = X.nonzero()[1]\n",
        "\n",
        "            # Get suspicious terms (positive coefficients for scam class)\n",
        "            suspicious_indices = []\n",
        "            for idx in feature_indices:\n",
        "                if coefficients[idx] > 0:  # Positive coefficient indicates scam\n",
        "                    suspicious_indices.append((idx, coefficients[idx]))\n",
        "\n",
        "            # Sort by coefficient value and get top terms\n",
        "            suspicious_indices.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            suspicious_terms = []\n",
        "            for idx, coef in suspicious_indices[:top_n]:\n",
        "                term = feature_names[idx]\n",
        "                suspicious_terms.append(term)\n",
        "\n",
        "            return suspicious_terms\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting suspicious terms: {e}\")\n",
        "            return []\n",
        "\n",
        "    def generate_explanation(self, prediction, risk_score, suspicious_terms, language):\n",
        "        \"\"\"Generate explanation in appropriate language\"\"\"\n",
        "        explanations = {\n",
        "            'english': {\n",
        "                'scam': f\"This message appears to be a SCAM (Risk: {risk_score}%). Suspicious elements detected.\",\n",
        "                'legitimate': f\"This message appears to be LEGITIMATE (Risk: {risk_score}%). No major red flags detected.\"\n",
        "            },\n",
        "            'sinhala': {\n",
        "                'scam': f\"‡∂∏‡∑ô‡∂∏ ‡∂¥‡∂´‡∑í‡∑Ä‡∑í‡∂©‡∂∫ ‡∑Ä‡∂Ç‡∂†‡∂±‡∑í‡∂ö (‡∂Ö‡∑Ä‡∂Ø‡∑è‡∂±‡∂∏: {risk_score}%) ‡∂∂‡∑Ä ‡∂¥‡∑ô‡∂±‡∑ö. ‡∑É‡∑ê‡∂ö ‡∑É‡∑Ñ‡∑í‡∂≠ ‡∂Ö‡∂Ç‡∂ú ‡∑Ñ‡∂∏‡∑î‡∑Ä‡∑í‡∂∫.\",\n",
        "                'legitimate': f\"‡∂∏‡∑ô‡∂∏ ‡∂¥‡∂´‡∑í‡∑Ä‡∑í‡∂©‡∂∫ ‡∂±‡∑ì‡∂≠‡∑ä‚Äç‡∂∫‡∑è‡∂±‡∑î‡∂ö‡∑ñ‡∂Ω (‡∂Ö‡∑Ä‡∂Ø‡∑è‡∂±‡∂∏: {risk_score}%) ‡∂∂‡∑Ä ‡∂¥‡∑ô‡∂±‡∑ö. ‡∑Ä‡∑í‡∑Å‡∑è‡∂Ω ‡∂ª‡∂≠‡∑î ‡∂ö‡∑ú‡∂©‡∑í ‡∑Ñ‡∂∏‡∑î ‡∂±‡∑ú‡∑Ä‡∑í‡∂∫.\"\n",
        "            },\n",
        "            'tamil': {\n",
        "                'scam': f\"‡Æá‡Æ®‡Øç‡Æ§ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Æø ‡ÆÆ‡Øã‡Æö‡Æü‡Æø‡ÆØ‡Ææ‡Æï (‡ÆÜ‡Æ™‡Æ§‡Øç‡Æ§‡ØÅ: {risk_score}%) ‡Æ§‡ØÜ‡Æ∞‡Æø‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. ‡Æö‡Æ®‡Øç‡Æ§‡Øá‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ±‡Øç‡Æï‡ØÅ‡Æ∞‡Æø‡ÆØ ‡Æï‡ØÇ‡Æ±‡ØÅ‡Æï‡Æ≥‡Øç ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ©.\",\n",
        "                'legitimate': f\"‡Æá‡Æ®‡Øç‡Æ§ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Æø ‡Æö‡Æü‡Øç‡Æü‡Æ™‡ØÇ‡Æ∞‡Øç‡Æµ‡ÆÆ‡Ææ‡Æ©‡Æ§‡Ææ‡Æï (‡ÆÜ‡Æ™‡Æ§‡Øç‡Æ§‡ØÅ: {risk_score}%) ‡Æ§‡ØÜ‡Æ∞‡Æø‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. ‡Æ™‡ØÜ‡Æ∞‡Æø‡ÆØ ‡Æö‡Æø‡Æµ‡Æ™‡Øç‡Æ™‡ØÅ ‡Æï‡Øä‡Æü‡Æø‡Æï‡Æ≥‡Øç ‡Æé‡Æ§‡ØÅ‡Æµ‡ØÅ‡ÆÆ‡Øç ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà.\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        base_explanation = explanations.get(language, explanations['english'])[prediction]\n",
        "\n",
        "        if suspicious_terms:\n",
        "            if language == 'english':\n",
        "                base_explanation += f\" Suspicious terms: {', '.join(suspicious_terms[:3])}\"\n",
        "            elif language == 'sinhala':\n",
        "                base_explanation += f\" ‡∑É‡∑ê‡∂ö ‡∑É‡∑Ñ‡∑í‡∂≠ ‡∑Ä‡∂†‡∂±: {', '.join(suspicious_terms[:3])}\"\n",
        "            elif language == 'tamil':\n",
        "                base_explanation += f\" ‡Æö‡Æ®‡Øç‡Æ§‡Øá‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ±‡Øç‡Æï‡ØÅ‡Æ∞‡Æø‡ÆØ ‡Æö‡Øä‡Æ±‡Øç‡Æï‡Æ≥‡Øç: {', '.join(suspicious_terms[:3])}\"\n",
        "\n",
        "        return base_explanation\n",
        "\n",
        "# Initialize and train the detector\n",
        "detector = MultilingualScamDetector()\n",
        "training_results = detector.train_all_models(processed_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFma0Qhh-AKj",
        "outputId": "ce5211bd-1555-4c2d-8869-1720377d28ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for english...\n",
            "‚úÖ English model trained - Accuracy: 1.000\n",
            "Training model for sinhala...\n",
            "‚úÖ Sinhala model trained - Accuracy: 1.000\n",
            "Training model for tamil...\n",
            "‚úÖ Tamil model trained - Accuracy: 1.000\n",
            "\n",
            "üéâ Training completed for all languages!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_messages = [\n",
        "    \"CONGRAT UR NUMBER IS SELECTED AS WINER OF 189,000 POUND IN THE U.N FUND RELIEF,TO GET UR WINING,EMAIL US UR,NAME,ADDRESS,NUM TO EMAIL:nationalun756@yahoo.com\",\n",
        "    \"‡∑É‡∑ë‡∂∏ ‡∂±‡∑í‡∑Ä‡∑É‡∂ö‡∂ß‡∂∏ ‡∂ª‡∑î‡∂¥‡∑í‡∂∫‡∂Ω‡∑ä 50,000 ‡∂ö ‡∂Ü‡∂∞‡∑è‡∂ª ‡∂∏‡∑î‡∂Ø‡∂Ω‡∂ö‡∑ä ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ì‡∂∏‡∂ß ‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í‡∑Ä‡∂ª‡∂∫‡∑è ‡∂Ö‡∂±‡∑î‡∂∏‡∑ê‡∂≠‡∑í‡∂∫ ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ì ‡∂≠‡∑í‡∂∂‡∑ô‡∂±‡∑Ä‡∑è.\",\n",
        "\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing the model:\\n\")\n",
        "\n",
        "for i, message in enumerate(test_messages, 1):\n",
        "    print(f\"--- Test {i} ---\")\n",
        "    result = detector.predict(message)\n",
        "\n",
        "    print(f\"Message: {result['text']}\")\n",
        "    print(f\"Language: {result['language']}\")\n",
        "    print(f\"Risk Score: {result['risk_score']}%\")\n",
        "    print(f\"Classification: {result['classification']}\")\n",
        "    print(f\"Suspicious Terms: {result['suspicious_terms']}\")\n",
        "    print(f\"Explanation: {result['explanation']}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDdwbtJb-Ukk",
        "outputId": "6e56c1bb-82da-42b9-866b-080bc8c32acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing the model:\n",
            "\n",
            "--- Test 1 ---\n",
            "Message: CONGRAT UR NUMBER IS SELECTED AS WINER OF 189,000 POUND IN THE U.N FUND RELIEF,TO GET UR WINING,EMAIL US UR,NAME,ADDRESS,NUM TO EMAIL:nationalun756@yahoo.com\n",
            "Language: english\n",
            "Risk Score: 44%\n",
            "Classification: legitimate\n",
            "Suspicious Terms: ['number']\n",
            "Explanation: This message appears to be LEGITIMATE (Risk: 44%). No major red flags detected. Suspicious terms: number\n",
            "\n",
            "--- Test 2 ---\n",
            "Message: ‡∑É‡∑ë‡∂∏ ‡∂±‡∑í‡∑Ä‡∑É‡∂ö‡∂ß‡∂∏ ‡∂ª‡∑î‡∂¥‡∑í‡∂∫‡∂Ω‡∑ä 50,000 ‡∂ö ‡∂Ü‡∂∞‡∑è‡∂ª ‡∂∏‡∑î‡∂Ø‡∂Ω‡∂ö‡∑ä ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ì‡∂∏‡∂ß ‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í‡∑Ä‡∂ª‡∂∫‡∑è ‡∂Ö‡∂±‡∑î‡∂∏‡∑ê‡∂≠‡∑í‡∂∫ ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ì ‡∂≠‡∑í‡∂∂‡∑ô‡∂±‡∑Ä‡∑è.\n",
            "Language: sinhala\n",
            "Risk Score: 49%\n",
            "Classification: legitimate\n",
            "Suspicious Terms: ['‡∂ö‡∑ä', '‡∂∏‡∂ß', '‡∂∏‡∂ß ']\n",
            "Explanation: ‡∂∏‡∑ô‡∂∏ ‡∂¥‡∂´‡∑í‡∑Ä‡∑í‡∂©‡∂∫ ‡∂±‡∑ì‡∂≠‡∑ä‚Äç‡∂∫‡∑è‡∂±‡∑î‡∂ö‡∑ñ‡∂Ω (‡∂Ö‡∑Ä‡∂Ø‡∑è‡∂±‡∂∏: 49%) ‡∂∂‡∑Ä ‡∂¥‡∑ô‡∂±‡∑ö. ‡∑Ä‡∑í‡∑Å‡∑è‡∂Ω ‡∂ª‡∂≠‡∑î ‡∂ö‡∑ú‡∂©‡∑í ‡∑Ñ‡∂∏‡∑î ‡∂±‡∑ú‡∑Ä‡∑í‡∂∫. ‡∑É‡∑ê‡∂ö ‡∑É‡∑Ñ‡∑í‡∂≠ ‡∑Ä‡∂†‡∂±: ‡∂ö‡∑ä, ‡∂∏‡∂ß, ‡∂∏‡∂ß \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_messages = [\n",
        "    \"CONGRAT UR NUMBER IS SELECTED AS WINER OF 189,000 POUND IN THE U.N FUND RELIEF,TO GET UR WINING,EMAIL US UR,NAME,ADDRESS,NUM TO EMAIL:nationalun756@yahoo.com\",\n",
        "    \"‡∑É‡∑ë‡∂∏ ‡∂±‡∑í‡∑Ä‡∑É‡∂ö‡∂ß‡∂∏ ‡∂ª‡∑î‡∂¥‡∑í‡∂∫‡∂Ω‡∑ä 50,000 ‡∂ö ‡∂Ü‡∂∞‡∑è‡∂ª ‡∂∏‡∑î‡∂Ø‡∂Ω‡∂ö‡∑ä ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ì‡∂∏‡∂ß ‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í‡∑Ä‡∂ª‡∂∫‡∑è ‡∂Ö‡∂±‡∑î‡∂∏‡∑ê‡∂≠‡∑í‡∂∫ ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ì ‡∂≠‡∑í‡∂∂‡∑ô‡∂±‡∑Ä‡∑è.\",\n",
        "    \"CONGRATS,UR NUMBER IS WINNER OF 189,000 POUND IN THE U.N FUND RELIEF PROGRAM,TO CLAIM UR WINNING,EMAIL US UR FULL NAME,ADDRES,NUM TO EMAIL:relieffund2@yahoo.com\",\n",
        "    \"You'll receive an otp code to redeem your cash prize of 100,00 rupees, click the link below and enter the otp you received in the website to get your cash prize\",\n",
        "\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing the model:\\n\")\n",
        "\n",
        "for i, message in enumerate(test_messages, 1):\n",
        "    print(f\"--- Test {i} ---\")\n",
        "    result = detector.predict(message)\n",
        "\n",
        "    print(f\"Message: {result['text']}\")\n",
        "    print(f\"Language: {result['language']}\")\n",
        "    print(f\"Risk Score: {result['risk_score']}%\")\n",
        "    print(f\"Classification: {result['classification']}\")\n",
        "    print(f\"Suspicious Terms: {result['suspicious_terms']}\")\n",
        "    print(f\"Explanation: {result['explanation']}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MGgvoJO-Y8z",
        "outputId": "8adf2dbb-a154-4f02-e242-710f66df5e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing the model:\n",
            "\n",
            "--- Test 1 ---\n",
            "Message: CONGRAT UR NUMBER IS SELECTED AS WINER OF 189,000 POUND IN THE U.N FUND RELIEF,TO GET UR WINING,EMAIL US UR,NAME,ADDRESS,NUM TO EMAIL:nationalun756@yahoo.com\n",
            "Language: english\n",
            "Risk Score: 44%\n",
            "Classification: legitimate\n",
            "Suspicious Terms: ['number']\n",
            "Explanation: This message appears to be LEGITIMATE (Risk: 44%). No major red flags detected. Suspicious terms: number\n",
            "\n",
            "--- Test 2 ---\n",
            "Message: ‡∑É‡∑ë‡∂∏ ‡∂±‡∑í‡∑Ä‡∑É‡∂ö‡∂ß‡∂∏ ‡∂ª‡∑î‡∂¥‡∑í‡∂∫‡∂Ω‡∑ä 50,000 ‡∂ö ‡∂Ü‡∂∞‡∑è‡∂ª ‡∂∏‡∑î‡∂Ø‡∂Ω‡∂ö‡∑ä ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ì‡∂∏‡∂ß ‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í‡∑Ä‡∂ª‡∂∫‡∑è ‡∂Ö‡∂±‡∑î‡∂∏‡∑ê‡∂≠‡∑í‡∂∫ ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ì ‡∂≠‡∑í‡∂∂‡∑ô‡∂±‡∑Ä‡∑è.\n",
            "Language: sinhala\n",
            "Risk Score: 49%\n",
            "Classification: legitimate\n",
            "Suspicious Terms: ['‡∂ö‡∑ä', '‡∂∏‡∂ß', '‡∂∏‡∂ß ']\n",
            "Explanation: ‡∂∏‡∑ô‡∂∏ ‡∂¥‡∂´‡∑í‡∑Ä‡∑í‡∂©‡∂∫ ‡∂±‡∑ì‡∂≠‡∑ä‚Äç‡∂∫‡∑è‡∂±‡∑î‡∂ö‡∑ñ‡∂Ω (‡∂Ö‡∑Ä‡∂Ø‡∑è‡∂±‡∂∏: 49%) ‡∂∂‡∑Ä ‡∂¥‡∑ô‡∂±‡∑ö. ‡∑Ä‡∑í‡∑Å‡∑è‡∂Ω ‡∂ª‡∂≠‡∑î ‡∂ö‡∑ú‡∂©‡∑í ‡∑Ñ‡∂∏‡∑î ‡∂±‡∑ú‡∑Ä‡∑í‡∂∫. ‡∑É‡∑ê‡∂ö ‡∑É‡∑Ñ‡∑í‡∂≠ ‡∑Ä‡∂†‡∂±: ‡∂ö‡∑ä, ‡∂∏‡∂ß, ‡∂∏‡∂ß \n",
            "\n",
            "--- Test 3 ---\n",
            "Message: CONGRATS,UR NUMBER IS WINNER OF 189,000 POUND IN THE U.N FUND RELIEF PROGRAM,TO CLAIM UR WINNING,EMAIL US UR FULL NAME,ADDRES,NUM TO EMAIL:relieffund2@yahoo.com\n",
            "Language: english\n",
            "Risk Score: 47%\n",
            "Classification: legitimate\n",
            "Suspicious Terms: ['claim', 'number']\n",
            "Explanation: This message appears to be LEGITIMATE (Risk: 47%). No major red flags detected. Suspicious terms: claim, number\n",
            "\n",
            "--- Test 4 ---\n",
            "Message: You'll receive an otp code to redeem your cash prize of 100,00 rupees, click the link below and enter the otp you received in the website to get your cash prize\n",
            "Language: english\n",
            "Risk Score: 64%\n",
            "Classification: scam\n",
            "Suspicious Terms: ['click', 'your', 'click link']\n",
            "Explanation: This message appears to be a SCAM (Risk: 64%). Suspicious elements detected. Suspicious terms: click, your, click link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save all models and vectorizers\n",
        "model_data = {\n",
        "    'models': detector.models,\n",
        "    'vectorizers': detector.vectorizers,\n",
        "    'preprocessor': detector.preprocessor\n",
        "}\n",
        "\n",
        "with open('clicksafe_multilingual_detector.pkl', 'wb') as f:\n",
        "    pickle.dump(model_data, f)\n",
        "\n",
        "print(\"‚úÖ Model saved as 'clicksafe_multilingual_detector.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9R1C0Az-eby",
        "outputId": "41cf32ca-7a16-4110-98ac-ac2785ccbaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model saved as 'clicksafe_multilingual_detector.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the .pkl file to your computer\n",
        "from google.colab import files\n",
        "\n",
        "# The file should already be created by your original code\n",
        "# If not, run this:\n",
        "# save_model_to_pkl(detector)\n",
        "\n",
        "# Download the file\n",
        "files.download('clicksafe_multilingual_detector.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "n4Govb_u-sx8",
        "outputId": "3ffbb18f-2de0-42cc-b279-b1f9601ace63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fd3be63c-e0d3-4738-8106-ad325ea2bf51\", \"clicksafe_multilingual_detector.pkl\", 114376)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_g6FfflW_EQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}