"""
Create a placeholder ML model for QR URL Safety Analysis
This creates a basic model structure that can be replaced with your trained model
"""
import pickle
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
import logging

logger = logging.getLogger(__name__)

def create_placeholder_model():
    """Create a placeholder model for QR URL safety analysis"""
    
    # Sample data for demonstration (replace with your actual training data)
    sample_urls = [
        # Safe URLs
        "https://www.google.com",
        "https://www.facebook.com",
        "https://www.amazon.com",
        "https://www.microsoft.com",
        "https://www.apple.com",
        "https://github.com/user/repo",
        "https://stackoverflow.com/questions",
        "https://www.wikipedia.org",
        # Suspicious URLs
        "http://secure-banking-update.com",
        "https://paypal-security-update.net",
        "http://amazon-prize-winner.org",
        "https://microsoft-account-verify.com",
        "http://urgent-account-suspension.net",
        "https://free-money-click-here.com",
        "http://192.168.1.1/phishing",
        "https://bit.ly/suspicious-link"
    ]
    
    # Labels (0 = safe, 1 = malicious)
    labels = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
    label_names = ["safe", "malicious"]
    
    print("üîß Creating placeholder QR URL Safety Model...")
    
    try:
        # Create and fit TF-IDF vectorizer
        vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            lowercase=True,
            stop_words='english'
        )
        
        # Transform URLs to feature vectors
        X_text = vectorizer.fit_transform(sample_urls)
        
        # Create additional numerical features
        numerical_features = []
        for url in sample_urls:
            features = [
                len(url),  # URL length
                len(url.split('.')),  # Domain parts
                len(url.split('/')),  # Path parts
                url.count('-'),  # Hyphen count
                int('http://' in url),  # Non-HTTPS
                int(any(word in url.lower() for word in ['secure', 'update', 'verify', 'urgent'])),  # Suspicious keywords
                url.count('?'),  # Query parameters
                int(any(c.isdigit() for c in (url.split('/')[2] if len(url.split('/')) > 2 else ''))),  # Domain has numbers
                len([c for c in url if not c.isalnum() and c not in '.-_~:/?#[]@!$&\'()*+,;='])  # Special chars
            ]
            numerical_features.append(features)
        
        # Combine text and numerical features
        X_numerical = np.array(numerical_features)
        X_combined = np.hstack([X_text.toarray(), X_numerical])
        
        # Create label encoder
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(labels)
        
        # Train Random Forest model
        model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            max_depth=10,
            min_samples_split=2,
            min_samples_leaf=1
        )
        
        # Fit the model
        model.fit(X_combined, y_encoded)
        
        # Test the model
        predictions = model.predict(X_combined)
        probabilities = model.predict_proba(X_combined)
        
        accuracy = np.mean(predictions == y_encoded)
        print(f"‚úÖ Model trained with accuracy: {accuracy:.2%}")
        
        # Create model package
        model_data = {
            'model': model,
            'vectorizer': vectorizer,
            'label_encoder': label_encoder,
            'feature_names': [
                'url_length', 'domain_parts', 'path_parts', 'hyphen_count',
                'non_https', 'suspicious_keywords', 'query_params', 
                'domain_has_numbers', 'special_char_count'
            ],
            'version': '2.0.0',
            'created_at': '2025-09-24',
            'sample_size': len(sample_urls),
            'accuracy': accuracy
        }
        
        # Create the model file
        model_path = 'H:/App-Project - 2 - Copy/clicksafe-api/qr_url_safety_model.pkl'
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"‚úÖ Placeholder model saved to: {model_path}")
        print("üìù Replace this with your actual trained model for production use")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error creating placeholder model: {str(e)}")
        return False

if __name__ == "__main__":
    create_placeholder_model()